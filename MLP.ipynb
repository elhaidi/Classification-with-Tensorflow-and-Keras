{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP : initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-e842b75513c1>:4: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\Administrateur.000\\Anaconda3\\envs\\tensor_env\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\Administrateur.000\\Anaconda3\\envs\\tensor_env\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Administrateur.000\\Anaconda3\\envs\\tensor_env\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Administrateur.000\\Anaconda3\\envs\\tensor_env\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Administrateur.000\\Anaconda3\\envs\\tensor_env\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "# https://gist.github.com/greydanus/f6eee59eaf1d90fcb3b534a25362cea4\n",
    "# https://stackoverflow.com/a/14434334\n",
    "# this function is used to update the plots for each epoch and error\n",
    "def plt_dynamic(x, y, y_1, ax, ticks,title, colors=['b']):\n",
    "    ax.plot(x, y, 'b', label=\"Train Loss\")\n",
    "    ax.plot(x, y_1, 'r', label=\"Test Loss\")\n",
    "    if len(x)==1:\n",
    "        plt.legend()\n",
    "        plt.title(title)\n",
    "    plt.yticks(ticks)\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "n_hidden_1 = 512 # 1st layer number of neurons\n",
    "n_hidden_2 = 128 # 2nd layer number of neurons\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We want to be able to input any number of MNIST images, each flattened into a 784-dimensional vector. \n",
    "# We represent this as a 2-D tensor of floating-point numbers, with a shape X = [None, 784]. \n",
    "# (Here None means that a dimension can be of any length.)\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# keep_prob: we will be using these placeholders when we use dropouts, while testing model\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "# keep_prob_input: we will be using these placeholders when we use dropouts, while training model\n",
    "keep_prob_input = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Administrateur.000\\Anaconda3\\envs\\tensor_env\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# Weight initialization\n",
    "\n",
    "# If we sample weights from a normal distribution N(0,σ) we satisfy this condition with σ=√(2/(ni+ni+1). \n",
    "# h1 =>  σ=√(2/(fan_in+fan_out+1) = 0.039  => N(0,σ) = N(0,0.039)\n",
    "# h2 =>  σ=√(2/(fan_in+fan_out+1) = 0.055  => N(0,σ) = N(0,0.055)\n",
    "# out =>  σ=√(2/(fan_in+fan_out+1) = 0.120  => N(0,σ) = N(0,0.120)\n",
    "# SGD: Xavier/Glorot Normal initialization.\n",
    "weights_sgd = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1],stddev=0.039, mean=0)),    #784x512 # sqrt(2/(784+512)) = 0.039\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2],stddev=0.055, mean=0)), #512x128 # sqrt(2/(512+128)) = 0.055\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes],stddev=0.120, mean=0))  #128x10\n",
    "}\n",
    "\n",
    "# for relu lates\n",
    "# If we sample weights from a normal distribution N(0,σ) we satisfy this condition with σ=√(2/(ni). \n",
    "# h1 =>  σ=√(2/(fan_in+1) = 0.062  => N(0,σ) = N(0,0.062)\n",
    "# h2 =>  σ=√(2/(fan_in+1) = 0.125  => N(0,σ) = N(0,0.125)\n",
    "# out =>  σ=√(2/(fan_in+1) = 0.120  => N(0,σ) = N(0,0.120)\n",
    "# He Normal initialization.\n",
    "weights_relu = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1],stddev=0.062, mean=0)),    #784x512\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2],stddev=0.125, mean=0)), #512x128\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes],stddev=0.120, mean=0))  #128x10\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),             #512x1\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),             #128x1\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))              #10x1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "training_epochs = 15\n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "display_step = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3> Model 1: input (784) - sigmoid(512) - sigmoid(128) - \n",
    "softmax(output 10) </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Use tf.matmul instead of \"*\" because tf.matmul can change it's dimensions on the fly (broadcast)\n",
    "    print( 'x:', x.get_shape(), 'W[h1]:', weights['h1'].get_shape(), 'b[h1]:', biases['b1'].get_shape())        \n",
    "    \n",
    "    # Hidden layer with Sigmoid activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1']) #(x*weights['h1']) + biases['b1']\n",
    "    layer_1 = tf.nn.sigmoid(layer_1)\n",
    "    print( 'layer_1:', layer_1.get_shape(), 'W[h2]:', weights['h2'].get_shape(), 'b[h2]:', biases['b2'].get_shape())        \n",
    "    \n",
    "    # Hidden layer with Sigmoid activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2']) # (layer_1 * weights['h2']) + biases['b2'] \n",
    "    layer_2 = tf.nn.sigmoid(layer_2)\n",
    "    print( 'layer_2:', layer_2.get_shape(), 'W[out]:', weights['out'].get_shape(), 'b3:', biases['out'].get_shape())        \n",
    "    \n",
    "    # Output layer with Sigmoid activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out'] # (layer_2 * weights['out']) + biases['out']    \n",
    "    out_layer = tf.nn.sigmoid(out_layer)\n",
    "    print('out_layer:',out_layer.get_shape())\n",
    "\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ Model 1 + AdamOptimizer __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Since we are using sigmoid activations in hiden layers we will be using weights that are initalized as weights_sgd\n",
    "y_sgd = multilayer_perceptron(x, weights_sgd, biases)\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits\n",
    "cost_sgd = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = y_sgd, labels = y_))\n",
    " \n",
    "optimizer_adam = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost_sgd)\n",
    "optimizer_sgdc = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost_sgd)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    fig,ax = plt.subplots(1,1)\n",
    "    ax.set_xlabel('epoch') ; ax.set_ylabel('Soft Max Cross Entropy loss')\n",
    "    xs, ytrs, ytes = [], [], []\n",
    "    for epoch in range(training_epochs):\n",
    "        train_avg_cost = 0.\n",
    "        test_avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # here c: corresponds to the parameter cost_sgd\n",
    "            # w : correspondse to the parameter weights_sgd\n",
    "            # c = sess.run() return the cost after every bath during train\n",
    "            # w = sess.run() return the weights that are modified after every batch through Back prop\n",
    "            # w is dict w = {'h1': updated h1 weight vector after the current batch,\n",
    "            #                'h2': updated h2 weight vector after the current batch, \n",
    "            #                'out': updated output weight vector after the current batch, \n",
    "            #                }\n",
    "            # you check these w matrix for every iteration, and check whats happening during back prop\n",
    "            #\n",
    "            # note: sess.run() returns parameter values based on the input parameters\n",
    "            # _, c, w = sess.run([optimizer_adam, cost_sgd,weights_sgd]) it returns three parameters\n",
    "            # _, c = sess.run([optimizer_adam, cost_sgd ]) it returns two parameters\n",
    "            # _ = sess.run([optimizer_adam]) it returns one paramter (for the input optimizer it return none)\n",
    "            # c = sess.run([cost_sgd]) it returns one paramter (for the input cost return error after the batch)\n",
    "\n",
    "            # feed_dict={x: batch_xs, y_: batch_ys} here x, y_ should be placeholders\n",
    "            # x, y_ are the input parameters on which the models gets trained\n",
    "\n",
    "            # here we use AdamOptimizer\n",
    "            _, c, w = sess.run([optimizer_adam, cost_sgd,weights_sgd], feed_dict={x: batch_xs, y_: batch_ys})\n",
    "            train_avg_cost += c / total_batch\n",
    "            c = sess.run(cost_sgd, feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
    "            test_avg_cost += c / total_batch\n",
    "\n",
    "        xs.append(epoch)\n",
    "        ytrs.append(train_avg_cost)\n",
    "        ytes.append(test_avg_cost)\n",
    "        plt_dynamic(xs, ytrs, ytes, ax, np.arange(1.3, 1.8, step=0.04), \"input-sigmoid(512)-sigmoid(128)-sigmoid(output)-AdamOptimizer\")\n",
    "\n",
    "        if epoch%display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"train cost={:.9f}\".format(train_avg_cost), \"test cost={:.9f}\".format(test_avg_cost))\n",
    "    plt_dynamic(xs, ytrs, ytes, ax, np.arange(1.3, 1.8, step=0.04), \"input-sigmoid(512)-sigmoid(128)-sigmoid(output)-AdamOptimizer\")\n",
    "\n",
    "    # we are calculating the final accuracy on the test data\n",
    "    correct_prediction = tf.equal(tf.argmax(y_sgd,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensor_env]",
   "language": "python",
   "name": "conda-env-tensor_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
